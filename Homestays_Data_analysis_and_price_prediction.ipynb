{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMImhRIR7g8qgwjWp+2gJKJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sameer103/Homestays_Data_analysis_and_price_prediction/blob/main/Homestays_Data_analysis_and_price_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EUvJPebe6mwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL3B8Nnrox2h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import pearsonr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta"
      ],
      "metadata": {
        "id": "OKzv3Z1_KM8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded=files.upload()"
      ],
      "metadata": {
        "id": "j0yZbpJWo9ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_excel(io.BytesIO(uploaded['Homestays_Data.xlsx']))"
      ],
      "metadata": {
        "id": "VWFRJ2r9pk5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "x5df6EyXH-Pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "xAb-8G9_IEol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task: Enhance the dataset by creating actionable and insightful features. Calculate `Host_Tenure` by determining the number of years from `host_since` to the current date, providing a measure of host experience. Generate `Amenities_Count` by counting the items listed in the `amenities` array to quantify property offerings. Determine `Days_Since_Last_Review` by calculating the days between `last_review` and today to assess listing activity and relevance."
      ],
      "metadata": {
        "id": "w5ATKfnjIshc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Host tenure calculation\n",
        "df['host_since'] = pd.to_datetime(df['host_since'], format='%m-%d-%Y', errors='coerce')\n",
        "df['current_date'] = datetime.now()\n",
        "host_tenure = ((df['current_date'] - df['host_since']).dt.days // 365).fillna(0).astype(int)\n",
        "host_tenure"
      ],
      "metadata": {
        "id": "3UBwc2YjIV4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if df['amenities'].dtype != 'str':\n",
        "    df['amenities'] = df['amenities'].astype('str')\n",
        "df['Amenities_Count'] = df['amenities'].apply(lambda x: len(set(x.strip('{}').split(','))))\n",
        "df[\"Amenities_Count\"]"
      ],
      "metadata": {
        "id": "v2IZCVmkLtb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['last_review'] = pd.to_datetime(df['last_review'], format='%d-%m-%Y', errors='coerce')\n",
        "days_since_last_review = ((df['current_date'] - df['last_review']).dt.days).fillna(0).astype(int)\n",
        "days_since_last_review"
      ],
      "metadata": {
        "id": "-RIefXSVLxgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['room_type'] = df['room_type'].astype('category').cat.codes\n",
        "df['property_type'] = df['property_type'].astype('category').cat.codes\n",
        "\n",
        "# Correlation analysis\n",
        "corr_matrix = df[['log_price', 'accommodates', 'number_of_reviews', 'room_type', 'property_type']].corr()\n",
        "print(corr_matrix)\n",
        "\n",
        "# Visualise correlation matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='YlOrRd')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R5qUR6ZWYWvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "df['log_price'].hist(bins=30)\n",
        "plt.title('Distribution of log_price')\n",
        "plt.subplot(1, 3, 2)\n",
        "df['accommodates'].hist(bins=30)\n",
        "plt.title('Distribution of accommodates')\n",
        "plt.subplot(1, 3, 3)\n",
        "df['number_of_reviews'].hist(bins=30)\n",
        "plt.title('Distribution of number_of_reviews')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TR1S-Ez5hOUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Scatter plot analysis\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(df['accommodates'], df['log_price'])\n",
        "plt.xlabel('Accommodates')\n",
        "plt.ylabel('log_price')\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(df['number_of_reviews'], df['log_price'])\n",
        "plt.xlabel('Number of Reviews')\n",
        "plt.ylabel('log_price')\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(df['room_type'], df['log_price'])\n",
        "plt.xlabel('Room Type')\n",
        "plt.ylabel('log_price')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xjHo8CPlhIJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task: Investigate the geographical data to understand regional pricing trends. Plot listings on a map using `latitude` and `longitude` data to visually assess price distribution. Examine if certain neighbourhoods or proximity to city centres influence pricing, providing a spatial perspective to the pricing strategy."
      ],
      "metadata": {
        "id": "bj3hEucPPqVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import folium\n",
        "from folium.plugins import HeatMap"
      ],
      "metadata": {
        "id": "NMpuzUBnPsLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot listings on a map\n",
        "m = folium.Map(location=[df['latitude'].mean(), df['longitude'].mean()], zoom_start=12)\n",
        "\n",
        "# Add markers for each listing\n",
        "for idx, row in df.iterrows():\n",
        "    folium.Marker([row['latitude'], row['longitude']], popup=f\"Price: {row['log_price']}\").add_to(m)\n",
        "\n",
        "m.save('listings_map.html')\n",
        "\n",
        "# Heatmap of price distribution\n",
        "plt.figure(figsize=(12, 8))\n",
        "HeatMap(data=df[['latitude', 'longitude', 'log_price']], radius=10).add_to(m)\n",
        "m.save('price_heatmap.html')\n",
        "\n"
      ],
      "metadata": {
        "id": "mVfDrHB0P8dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze price distribution by neighborhood\n",
        "df['neighborhood'] = df.apply(lambda row: f\"{row['city']}\", axis=1)\n",
        "neighborhood_prices = df.groupby('neighborhood')['log_price'].mean().sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "neighborhood_prices.plot(kind='bar')\n",
        "plt.title('Average log_price by Neighborhood')\n",
        "plt.xlabel('Neighborhood')\n",
        "plt.ylabel('log_price')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "_WyZ7wRjQ7ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze price distribution by distance to city centre\n",
        "df['distance_to_city'] = np.sqrt((df['latitude'])**2 + (df['longitude'])**2)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(df['distance_to_city'], df['log_price'])\n",
        "plt.title('log_price vs Distance to City Centre')\n",
        "plt.xlabel('Distance to City Centre')\n",
        "plt.ylabel('log_price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zoCM_oIrUewF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task: Apply advanced natural language processing techniques to the `description` texts to extract sentiment scores. Use sentiment analysis tools to determine whether positive or negative descriptions influence listing prices, incorporating these findings into the predictive model being trained as a feature."
      ],
      "metadata": {
        "id": "MqYhFKgDU7Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "id": "yo-odqXAVO8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.linear_model import LinearRegression\n"
      ],
      "metadata": {
        "id": "qS32qT0YU9AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment analysis using TextBlob\n",
        "df['description_polarity'] = df['description'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
        "df['description_subjectivity'] = df['description'].apply(lambda x: TextBlob(str(x)).sentiment.subjectivity)\n",
        "\n",
        "# Sentiment analysis using VADER\n",
        "vader = SentimentIntensityAnalyzer()\n",
        "df['description_vader_score'] = df['description'].apply(lambda x: vader.polarity_scores(str(x))['compound'])\n",
        "\n",
        "# Explore the sentiment scores\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "df['description_polarity'].hist(bins=30)\n",
        "plt.title('TextBlob Polarity')\n",
        "plt.subplot(1, 3, 2)\n",
        "df['description_subjectivity'].hist(bins=30)\n",
        "plt.title('TextBlob Subjectivity')\n",
        "plt.subplot(1, 3, 3)\n",
        "df['description_vader_score'].hist(bins=30)\n",
        "plt.title('VADER Sentiment Score')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze the relationship between sentiment and log_price\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(df['description_polarity'], df['log_price'])\n",
        "plt.xlabel('TextBlob Polarity')\n",
        "plt.ylabel('log_price')\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(df['description_subjectivity'], df['log_price'])\n",
        "plt.xlabel('TextBlob Subjectivity')\n",
        "plt.ylabel('log_price')\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(df['description_vader_score'], df['log_price'])\n",
        "plt.xlabel('VADER Sentiment Score')\n",
        "plt.ylabel('log_price')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xU_nuSEzVEb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Incorporate sentiment features into the predictive model\n",
        "X = df[['accommodates', 'number_of_reviews', 'description_polarity', 'description_subjectivity', 'description_vader_score']]\n",
        "y = df['log_price']\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "print('Model Coefficients:', model.coef_)"
      ],
      "metadata": {
        "id": "k_nVlOJVblqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task: Thoroughly parse and analyse the `amenities` provided in the listings. Identify which amenities are most associated with higher or lower prices by applying statistical tests to determine correlations, thereby informing both pricing strategy and model inputs."
      ],
      "metadata": {
        "id": "vfxLyVotWAif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr, chi2_contingency\n"
      ],
      "metadata": {
        "id": "PuvdV5ijWBle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse the amenities\n",
        "df['amenities'] = df['amenities'].apply(lambda x: set(x.strip('{}').split(',')))\n",
        "all_amenities = set([item for row in df['amenities'] for item in row])\n",
        "\n",
        "# Create a binary amenity matrix\n",
        "amenity_matrix = pd.DataFrame(0, index=df.index, columns=all_amenities)\n",
        "for i, row in df.iterrows():\n",
        "    for amenity in row['amenities']:\n",
        "        amenity_matrix.at[i, amenity] = 1\n",
        "\n",
        "# Analyze the relationship between amenities and log_price\n",
        "plt.figure(figsize=(12, 6))\n",
        "for amenity in all_amenities:\n",
        "    pearson_r, p_value = pearsonr(amenity_matrix[amenity], df['log_price'])\n",
        "    if p_value < 0.05:\n",
        "        plt.scatter(amenity_matrix[amenity], df['log_price'], label=amenity, alpha=0.5)\n",
        "plt.xlabel('Amenity Presence')\n",
        "plt.ylabel('log_price')\n",
        "plt.title('Amenities vs. log_price')\n",
        "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Identify the most influential amenities\n",
        "corr_matrix = amenity_matrix.corrwith(df['log_price'])\n",
        "top_amenities = corr_matrix.abs().sort_values(ascending=False).head(10)\n",
        "print('Top 10 Amenities Correlated with log_price:')\n",
        "print(top_amenities)\n",
        "\n",
        "# Perform chi-square test for association\n",
        "for amenity in all_amenities:\n",
        "    contingency_table = pd.crosstab(df['log_price'].apply(lambda x: 'High' if x > df['log_price'].mean() else 'Low'), amenity_matrix[amenity])\n",
        "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "    if p_value < 0.05:\n",
        "        print(f\"Amenity '{amenity}' is significantly associated with high/low log_price (p-value: {p_value:.4f})\")"
      ],
      "metadata": {
        "id": "nrRKIDB1WSTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task: Convert categorical data into a format suitable for machine learning analysis. Apply one-hot encoding to variables like `room_type`, `city`, and `property_type`, ensuring that the model can interpret these as distinct features without any ordinal implication."
      ],
      "metadata": {
        "id": "Hek9IRmkXC98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n"
      ],
      "metadata": {
        "id": "_y9LIR7JXBxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = OneHotEncoder()\n",
        "\n",
        "# Encode room_type\n",
        "room_type_encoded = encoder.fit_transform(df['room_type'].to_numpy().reshape(-1, 1)).toarray()\n",
        "room_type_df = pd.DataFrame(room_type_encoded, columns=[f'room_type_{category}' for category in encoder.categories_[0]])\n",
        "df = pd.concat([df, room_type_df], axis=1)\n",
        "\n",
        "# Encode city\n",
        "city_encoded = encoder.fit_transform(df['city'].to_numpy().reshape(-1, 1)).toarray()\n",
        "city_df = pd.DataFrame(city_encoded, columns=[f'city_{category}' for category in encoder.categories_[0]])\n",
        "df = pd.concat([df, city_df], axis=1)\n",
        "\n",
        "# Encode property_type\n",
        "property_type_encoded = encoder.fit_transform(df['property_type'].to_numpy().reshape(-1, 1)).toarray()\n",
        "property_type_df = pd.DataFrame(property_type_encoded, columns=[f'property_type_{category}' for category in encoder.categories_[0]])\n",
        "df = pd.concat([df, property_type_df], axis=1)\n",
        "\n",
        "# Drop the original categorical columns\n",
        "df = df.drop(['room_type', 'city', 'property_type'], axis=1)\n",
        "\n",
        "# Inspect the new DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "OqdB3CRZXRRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task: Design and train predictive models to estimate `log_price`. Begin with a simple linear regression to establish a baseline, then explore more complex models such as RandomForest and GradientBoosting to better capture non-linear relationships and interactions between features. Document (briefly within Jupyter notebook itself) the model-building process, specifying the choice of algorithms and rationale."
      ],
      "metadata": {
        "id": "EboeEFGzXZba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "7hmdEY1cXT1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "X = df[['accommodates', 'number_of_reviews', 'description_polarity', 'description_subjectivity', 'description_vader_score']]\n",
        "y = df['log_price']\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Linear Regression model\n",
        "print('Training a Linear Regression model...')\n",
        "linear_reg = LinearRegression()\n",
        "linear_reg.fit(X_train, y_train)\n",
        "linear_reg_train_mse = mean_squared_error(y_train, linear_reg.predict(X_train))\n",
        "linear_reg_test_mse = mean_squared_error(y_test, linear_reg.predict(X_test))\n",
        "linear_reg_train_r2 = r2_score(y_train, linear_reg.predict(X_train))\n",
        "linear_reg_test_r2 = r2_score(y_test, linear_reg.predict(X_test))\n",
        "print(f'Linear Regression Train MSE: {linear_reg_train_mse:.4f}')\n",
        "print(f'Linear Regression Test MSE: {linear_reg_test_mse:.4f}')\n",
        "print(f'Linear Regression Train R-squared: {linear_reg_train_r2:.4f}')\n",
        "print(f'Linear Regression Test R-squared: {linear_reg_test_r2:.4f}')\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "print('\\nTraining a Random Forest Regressor model...')\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_reg_train_mse = mean_squared_error(y_train, rf_reg.predict(X_train))\n",
        "rf_reg_test_mse = mean_squared_error(y_test, rf_reg.predict(X_test))\n",
        "rf_reg_train_r2 = r2_score(y_train, rf_reg.predict(X_train))\n",
        "rf_reg_test_r2 = r2_score(y_test, rf_reg.predict(X_test))\n",
        "print(f'Random Forest Regressor Train MSE: {rf_reg_train_mse:.4f}')\n",
        "print(f'Random Forest Regressor Test MSE: {rf_reg_test_mse:.4f}')\n",
        "print(f'Random Forest Regressor Train R-squared: {rf_reg_train_r2:.4f}')\n",
        "print(f'Random Forest Regressor Test R-squared: {rf_reg_test_r2:.4f}')\n",
        "\n",
        "# Train a Gradient Boosting Regressor\n",
        "print('\\nTraining a Gradient Boosting Regressor model...')\n",
        "gb_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "gb_reg.fit(X_train, y_train)\n",
        "gb_reg_train_mse = mean_squared_error(y_train, gb_reg.predict(X_train))\n",
        "gb_reg_test_mse = mean_squared_error(y_test, gb_reg.predict(X_test))\n",
        "gb_reg_train_r2 = r2_score(y_train, gb_reg.predict(X_train))\n",
        "gb_reg_test_r2 = r2_score(y_test, gb_reg.predict(X_test))\n",
        "print(f'Gradient Boosting Regressor Train MSE: {gb_reg_train_mse:.4f}')\n",
        "print(f'Gradient Boosting Regressor Test MSE: {gb_reg_test_mse:.4f}')\n",
        "print(f'Gradient Boosting Regressor Train R-squared: {gb_reg_train_r2:.4f}')\n",
        "print(f'Gradient Boosting Regressor Test R-squared: {gb_reg_test_r2:.4f}')"
      ],
      "metadata": {
        "id": "SHIqhdgMXkhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task: Analyze the trained models to identify which features most significantly impact `log_price`. Utilize model-specific methods like feature importance scores for tree-based models and SHAP values for an in-depth understanding of feature contributions."
      ],
      "metadata": {
        "id": "R8zQMu7wYJVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn-extra"
      ],
      "metadata": {
        "id": "731PT3FKYg_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "#import plot_partial_dependence\n",
        "import shap"
      ],
      "metadata": {
        "id": "N-rslEZSYU5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['accommodates', 'number_of_reviews', 'room_type', 'city', 'property_type', 'description_polarity', 'description_subjectivity', 'description_vader_score']]\n",
        "y = df['log_price']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the models\n",
        "linear_reg = LinearRegression()\n",
        "linear_reg.fit(X_train, y_train)\n",
        "\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "gb_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "gb_reg.fit(X_train, y_train)\n",
        "\n",
        "# Feature importance for Random Forest Regressor\n",
        "plt.figure(figsize=(12, 6))\n",
        "feature_importances = rf_reg.feature_importances_\n",
        "feature_names = X.columns\n",
        "sorted_idx = np.argsort(feature_importances)[::-1]\n",
        "\n",
        "plt.bar(range(X.shape[1]), feature_importances[sorted_idx])\n",
        "plt.xticks(range(X.shape[1]), [feature_names[i] for i in sorted_idx], rotation=90)\n",
        "plt.title('Random Forest Feature Importances')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# SHAP values for Gradient Boosting Regressor\n",
        "explainer = shap.TreeExplainer(gb_reg)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
        "plt.title('SHAP Values for Gradient Boosting Regressor')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6O7cKZoRZWCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task: Critically evaluate the performance of the final model on a reserved test set. Use metrics such as Root Mean Squared Error (RMSE) and R-squared to assess accuracy and goodness of fit. Provide a detailed analysis of the residuals to check for any patterns that might suggest model biases or misfit.\n"
      ],
      "metadata": {
        "id": "basoIY0_ZXBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n"
      ],
      "metadata": {
        "id": "LULSdxAMZdej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['accommodates', 'number_of_reviews', 'description_polarity', 'description_subjectivity', 'description_vader_score']]\n",
        "y = df['log_price']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the final model (Gradient Boosting Regressor)\n",
        "gb_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "gb_reg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = gb_reg.predict(X_test)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "test_r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'Test RMSE: {test_rmse:.4f}')\n",
        "print(f'Test R-squared: {test_r2:.4f}')\n",
        "\n",
        "# Analyze the residuals\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.scatterplot(x=y_test, y=y_pred)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.xlabel('Actual log_price')\n",
        "plt.ylabel('Predicted log_price')\n",
        "plt.title('Actual vs. Predicted log_price')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "residuals = y_test - y_pred\n",
        "sns.histplot(residuals, bins=30)\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Residuals')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hQnRf6vxZqny"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}